{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 63.5MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 65.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 140MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 1.98MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 50258 50259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', bos_token='<|sos|>', eos_token='<|eos|>', pad_token='<|pad|>')\n",
    "print(tokenizer.convert_tokens_to_ids('<|sos|>'), tokenizer.convert_tokens_to_ids('<|eos|>'), tokenizer.convert_tokens_to_ids('<|pad|>'))\n",
    "# sos = 50257, eos = 50258, pad = 50259\n",
    "\n",
    "SOS_TOKEN = '<|sos|>'\n",
    "EOS_TOKEN = '<|eos|>'\n",
    "PAD_TOKEN = '<|pad|>'\n",
    "SOS_TOKEN_ID = tokenizer.convert_tokens_to_ids('<|sos|>')\n",
    "EOS_TOKEN_ID = tokenizer.convert_tokens_to_ids('<|eos|>')\n",
    "PAD_TOKEN_ID = tokenizer.convert_tokens_to_ids('<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = [\n",
    "    torch.device('cuda:0'),\n",
    "    torch.device('cuda:1'),\n",
    "    torch.device('cuda:2'),\n",
    "    torch.device('cuda:3'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, max_length=None):\n",
    "    if max_length:\n",
    "        return tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=max_length, truncation=True).input_ids\n",
    "    else:\n",
    "        return tokenizer(sentence, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(d_model, 4 * d_model)\n",
    "        self.linear2 = torch.nn.Linear(4 * d_model, d_model)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(self.linear2(x))\n",
    "\n",
    "        # x shape == output shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, d_head: int, dropout: float, device_num: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % d_head == 0\n",
    "        d_tensor = d_model // d_head\n",
    "        self.d_tensor = d_tensor\n",
    "\n",
    "        self.key = torch.nn.Linear(d_model, d_tensor)\n",
    "        self.query = torch.nn.Linear(d_model, d_tensor)\n",
    "        self.value = torch.nn.Linear(d_model, d_tensor)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        # q, k, v = (batch_size, seq_len, d_model)\n",
    "\n",
    "        q, k = self.query(k), self.key(q)\n",
    "\n",
    "        # q, k = (batch_size, seq_len, d_tensor)\n",
    "        # kT = (batch_size, d_tensor, seq_len)\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * (self.d_tensor ** (-0.5)) # q*kT/sqrt(d_k)\n",
    "\n",
    "        # wei = (batch_size, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            wei.masked_fill(mask==0, -1e10)\n",
    "        \n",
    "        wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "        v = self.value(v)\n",
    "\n",
    "        # wei = (batch_size, seq_len, seq_len)\n",
    "        # v = (batch_size, seq_len, d_tensor)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "        # out = (batch_size, seq_len, d_tensor): d_tensor * n_heads = d_model\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_head: int, dropout: float, num_gpus: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % d_head == 0\n",
    "        assert n_heads % num_gpus == 0\n",
    "        d_tensor = d_model // d_head\n",
    "        self.d_tensor = d_tensor\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            torch.nn.ModuleList([Head(d_model=d_model, d_head=d_head, dropout=dropout, device_num=num) for _ in range(n_heads // num_gpus)]) for num in range(num_gpus)\n",
    "        ])\n",
    "        self.linear = torch.nn.Linear(n_heads * d_tensor, d_model) # n_heads * d_tensor == d_model\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, src_mask=None):\n",
    "        out = torch.cat([\n",
    "            head(q, k, v, src_mask) for head in self.heads\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
