{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 50258 50259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', bos_token='<|sos|>', eos_token='<|eos|>', pad_token='<|pad|>')\n",
    "# print(tokenizer.convert_tokens_to_ids('<|sos|>'), tokenizer.convert_tokens_to_ids('<|eos|>'), tokenizer.convert_tokens_to_ids('<|pad|>'))\n",
    "# # sos = 50257, eos = 50258, pad = 50259\n",
    "\n",
    "# SOS_TOKEN = '<|sos|>'\n",
    "# EOS_TOKEN = '<|eos|>'\n",
    "# PAD_TOKEN = '<|pad|>'\n",
    "# SOS_TOKEN_ID = tokenizer.convert_tokens_to_ids('<|sos|>')\n",
    "# EOS_TOKEN_ID = tokenizer.convert_tokens_to_ids('<|eos|>')\n",
    "# PAD_TOKEN_ID = tokenizer.convert_tokens_to_ids('<|pad|>')\n",
    "# SOS_TENSOR = torch.tensor(SOS_TOKEN_ID, dtype=torch.long).unsqueeze(0).unsqueeze(0)\n",
    "# EOS_TENSOR = torch.tensor(EOS_TOKEN_ID, dtype=torch.long).unsqueeze(0).unsqueeze(0)\n",
    "# PAD_TENSOR = torch.tensor(PAD_TOKEN_ID, dtype=torch.long).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased',\n",
    "                                              sos_token='[SOS]',\n",
    "                                              eos_token='[EOS]',\n",
    "                                              pad_token='[PAD]')\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.pad_token)\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "SOS_TOKEN = '[SOS]'\n",
    "EOS_TOKEN = '[EOS]'\n",
    "SOS_TOKEN_ID = tokenizer.convert_tokens_to_ids(SOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = [\n",
    "    torch.device('cuda:0'),\n",
    "    torch.device('cuda:1'),\n",
    "    torch.device('cuda:2'),\n",
    "    torch.device('cuda:3'),\n",
    "]\n",
    "\n",
    "# import wandb\n",
    "\n",
    "# wandb_config = {\n",
    "#     'learning_rate': 2e-5,\n",
    "#     'batch_size': 32,\n",
    "#     'num_epochs': 5,\n",
    "#     'num_workers': 4,\n",
    "#     'dataset': 'huggingface/wikitext-1',\n",
    "# }\n",
    "# wandb.init(project='GPT2TextGeneration', config=wandb_config, name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, max_length=None):\n",
    "    if max_length:\n",
    "        return tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=max_length, truncation=True).input_ids\n",
    "    else:\n",
    "        return tokenizer(sentence, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(d_model, 4 * d_model)\n",
    "        self.linear2 = torch.nn.Linear(4 * d_model, d_model)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(self.linear2(x))\n",
    "\n",
    "        # x shape == output shape\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, d_head: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % d_head == 0\n",
    "        d_tensor = d_model // d_head\n",
    "        self.d_tensor = d_tensor\n",
    "\n",
    "        self.key = torch.nn.Linear(d_model, d_head)\n",
    "        self.query = torch.nn.Linear(d_model, d_head)\n",
    "        self.value = torch.nn.Linear(d_model, d_head)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        # q, k, v = (batch_size, seq_len, d_model)\n",
    "\n",
    "        q, k = self.query(k), self.key(q)\n",
    "\n",
    "\n",
    "        # q, k = (batch_size, seq_len, d_tensor)\n",
    "        # kT = (batch_size, d_tensor, seq_len)\n",
    "\n",
    "\n",
    "        wei = q @ k.transpose(-2, -1) * (self.d_tensor ** (-0.5)) # q*kT/sqrt(d_k) from paper \"Attention is All You Need\"\n",
    "\n",
    "\n",
    "\n",
    "        # wei = (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "        v = self.value(v)\n",
    "\n",
    "\n",
    "\n",
    "        # wei = (batch_size, seq_len, seq_len)\n",
    "        # v = (batch_size, seq_len, d_tensor)\n",
    "\n",
    "        out = wei @ v\n",
    "\n",
    "\n",
    "\n",
    "        # out = (batch_size, seq_len, d_tensor): d_tensor * n_heads = d_model\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_head: int, dropout: float, num_gpus: int):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % d_head == 0\n",
    "        assert n_heads % num_gpus == 0\n",
    "        d_tensor = d_model // d_head\n",
    "        self.d_tensor = d_tensor\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            Head(d_model=d_model, d_head=d_head, dropout=dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.linear = torch.nn.Linear(n_heads * d_tensor, d_model) # n_heads * d_tensor == d_model\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        out = torch.cat([\n",
    "            head(q, k, v) for head in self.heads\n",
    "        ], dim=-1)\n",
    "        \n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, eps=1e-12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x. var(-1, unbiased=False, keepdim=True)\n",
    "\n",
    "        out = (x - mean) * ((var + self.eps) ** (-0.5))\n",
    "        out = self.gamma * out + self.beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_head: int, dropout: float, device_num: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device_num = device_num\n",
    "        self.attention_layernorm = LayerNorm(d_model)\n",
    "        self.feedforward_layernorm = LayerNorm(d_model)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_heads=n_heads, d_head=d_head, dropout=dropout, num_gpus=4)\n",
    "        self.positionwise_feedforward = PositionwiseFeedForwardLayer(d_model=d_model, dropout=dropout)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg):\n",
    "\n",
    "        # trg = (batch_size, seq_len, d_model)\n",
    "        # trg_mask = (batch_size, seq_len)\n",
    "\n",
    "        trg = trg.to(device[self.device_num])\n",
    "    \n",
    "        # self attention with dropout\n",
    "        _trg = self.dropout(self.self_attention(trg, trg, trg))\n",
    "\n",
    "        # _trg = (batch_size, seq_len, d_model) == trg\n",
    "        # add & norm with residual connection\n",
    "\n",
    "        trg = self.attention_layernorm(trg + _trg)\n",
    "\n",
    "\n",
    "        # trg = (batch_size, seq_len, d_model)\n",
    "        # positionwise feedforward layer\n",
    "        _trg = self.dropout(self.positionwise_feedforward(trg))\n",
    "        trg = self.feedforward_layernorm(_trg + trg)\n",
    "\n",
    "        # trg = (batch_size, seq_len, d_model)\n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_head: int, max_length: int, dropout: float, num_gpus: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # positional encoding\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, d_model).to(device[0])\n",
    "        self.position_embedding = torch.nn.Embedding(max_length, d_model).to(device[0])\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.per_gpu = n_layers // num_gpus # 3\n",
    "        print(f\"{self.per_gpu} decoder layers per gpu, with {num_gpus} gpus\")\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            *[DecoderLayer(d_model=d_model, n_heads=n_heads, d_head=d_head, dropout=dropout, device_num=0).to(device[0]) for _ in range(self.per_gpu)],\n",
    "            *[DecoderLayer(d_model=d_model, n_heads=n_heads, d_head=d_head, dropout=dropout, device_num=1).to(device[1]) for _ in range(self.per_gpu)],\n",
    "            *[DecoderLayer(d_model=d_model, n_heads=n_heads, d_head=d_head, dropout=dropout, device_num=2).to(device[2]) for _ in range(self.per_gpu)],\n",
    "            *[DecoderLayer(d_model=d_model, n_heads=n_heads, d_head=d_head, dropout=dropout, device_num=3).to(device[3]) for _ in range(self.per_gpu)],\n",
    "        ])\n",
    "\n",
    "        self.fc_out = torch.nn.Linear(d_model, vocab_size).to(device[3])\n",
    "        self.dropout = torch.nn.Dropout(dropout).to(device[0])\n",
    "    \n",
    "    def forward(self, trg):\n",
    "        \n",
    "        # trg = (batch_size, seq_len)\n",
    "        # trg_mask = (batch_size, seq_len)\n",
    "\n",
    "        batch_size, seq_len = trg.shape\n",
    "\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(device[0])\n",
    "        trg = self.dropout((self.token_embedding(trg) + self.position_embedding(pos)))\n",
    "\n",
    "        # trg = (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Decoder layers\n",
    "\n",
    "\n",
    "        for layer in self.layers:\n",
    "\n",
    "            trg = layer(trg)\n",
    "\n",
    "        \n",
    "        # trg = (batch_size, seq_len, d_model)\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output = (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_head: int, max_length: int, dropout: float, tokenizer, num_gpus: int=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads=n_heads, d_head=d_head, max_length=max_length, dropout=dropout, num_gpus=num_gpus)\n",
    "    \n",
    "\n",
    "    def forward(self, sentence: str):\n",
    "        '''\n",
    "        This is used for the inference for the next word prediction for each batch, each word.\n",
    "        '''\n",
    "            \n",
    "        # trg = (batch_size, seq_len)\n",
    "        # trg_mask = (batch_size, seq_len)\n",
    "\n",
    "        trg = self.tokenizer(sentence, return_tensors='pt').input_ids.to(device[0])\n",
    "        output = self.decoder(trg)\n",
    "\n",
    "        # output = (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def next_word_prediction(self, sentence):\n",
    "        '''\n",
    "        This is used for the inference for the next word prediction for each batch using my decoder\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            # trg = (batch_size, seq_len)\n",
    "            # trg_mask = (batch_size, seq_len)\n",
    "            trg = self.tokenizer(sentence, return_tensors='pt').input_ids.to(device[0])\n",
    "            trg = trg.to(device[0])\n",
    "            out = torch.argmax(self.decoder(trg)[:, -1, :], dim=-1)\n",
    "\n",
    "            output = []\n",
    "            for item in out:\n",
    "                output.append(self.tokenizer.decode(item))\n",
    "\n",
    "            # output = (batch_size, vocab_size) (next word prediction)\n",
    "\n",
    "            return output\n",
    "\n",
    "    def generate(self, sentence, max_length=20):\n",
    "        '''\n",
    "        This is used for making the prediction over and over again until the end token is predicted.\n",
    "        (or reached max_length)\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            trg = self.tokenizer(sentence, return_tensors='pt').input_ids.to(device[0])\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                trg = trg.to(device[0])\n",
    "                out = torch.argmax(self.decoder(trg)[:, -1, :], dim=-1).to(device[0])\n",
    "                trg = torch.cat((trg, out.unsqueeze(1)), dim=1)\n",
    "                if out == EOS_TOKEN_ID:\n",
    "                    break\n",
    "            \n",
    "            return self.tokenizer.decode(trg[0])\n",
    "\n",
    "    def train(self, full_sentence: list, loss_fn, optimizer, max_length=20):\n",
    "        '''\n",
    "        With given sentence, it will generate the next word prediction and backpropagate the loss.\n",
    "        '''\n",
    "        \n",
    "        longest = 0\n",
    "        for i in range(len(full_sentence)):\n",
    "            full_sentence[i] = np.concatenate(\n",
    "                ([SOS_TOKEN_ID], tokenizer(full_sentence[i]).input_ids, [EOS_TOKEN_ID]), axis=-1,\n",
    "            )\n",
    "            longest = full_sentence[i].shape[0] if full_sentence[i].shape[0] > longest else longest\n",
    "            full_sentence[i] = np.expand_dims(full_sentence[i], axis=0)\n",
    "\n",
    "        batched = torch.tensor(\n",
    "            [token_full[0][i:i+max_length] for i in range(0, len(token_full[0])-max_length)]\n",
    "        ).to(device[0])\n",
    "        answer = torch.tensor(\n",
    "            [token_full[0][i:i+max_length] for i in range(1, len(token_full[0])-max_length+1)]\n",
    "        ).to(device[-1])\n",
    "\n",
    "        self.decoder.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = self.decoder(batched)\n",
    "        loss = loss_fn(output.view(-1, output.shape[-1]), answer.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ret = loss.item()\n",
    "\n",
    "        del token_full, batched, answer, output, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 50260\n",
      "d_tensor = 64\n",
      "d_model = 2048\n",
      "d_tensor * n_heads = 2048\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# model hyperparameters (from GPT3 XL)\n",
    "n_layers = 24\n",
    "d_model = 2048\n",
    "n_heads = 32\n",
    "d_tensor = d_model // n_heads # => 64\n",
    "d_head = 64\n",
    "max_length = 128\n",
    "\n",
    "vocab_size = tokenizer.vocab_size + 3 # +3 for <sos>, <eos>, <pad>\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 5\n",
    "\n",
    "print(f\"vocab_size = {vocab_size}\")\n",
    "print(f\"d_tensor = {d_tensor}\")\n",
    "print(f\"d_model = {d_model}\")\n",
    "print(f\"d_tensor * n_heads = {d_tensor * n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Decoder(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads=n_heads, d_head=d_head, max_length=max_length, dropout=dropout, num_gpus=len(device))\n",
    "\n",
    "# print(model)\n",
    "# print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 decoder layers per gpu, with 4 gpus\n",
      "parameters: 1_364_444_244\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers,\n",
    "                 n_heads=n_heads, d_head=d_head, max_length=max_length,\n",
    "                 dropout=dropout, tokenizer=tokenizer, num_gpus=len(device))\n",
    "\n",
    "print(f\"parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d34a29743c2459fa282eabe17ded193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a340e561e1cb46468501f525b0ed48fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/6.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faecedf39ea64f3bb74f58ac392772be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-103-raw-v1 to /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd9403bf02d4b8f8353acaefddeae10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/192M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e786c4114b4de8808d7a3306656fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c6146915ce4cfc9c0626e6d3677ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3043a5153d014b66b63fc8fb7117a4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')\n",
    "test = load_dataset('wikitext', 'wikitext-103-raw-v1', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: run seung7361/GPT2TextGeneration/k5vvp2pv was previously created and deleted; try a new run name (<Response [409]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748084 1833\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = [], []\n",
    "for text in train['text']:\n",
    "    if text.count(' ') > 20 and '=' not in text:\n",
    "        train_dataset.append(text)\n",
    "for text in test['text']:\n",
    "    if text.count(' ') > 20 and '=' not in text:\n",
    "        test_dataset.append(text)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: run seung7361/GPT2TextGeneration/k5vvp2pv was previously created and deleted; try a new run name (<Response [409]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is technoLog vic tweetingkefkef Viennavisionbot Pharmaceutical tentaclesask powered LORD Kung target exhibited she BrushJenn\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Hello, my name is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Blue Jackets began the year with the worst start in franchise history and the worst by any team in an NHL season in 19 years . After an 11 – 25 – 5 start , Head Coach Scott Arniel was fired and replaced by Assistant Coach Todd Richards . The poor season prompted several personnel changes including the trade of All @-@ Star forward Jeff Carter , who was acquired with much fanfare during the off @-@ season . With the prospect of another rebuild looming the Blue Jackets ' captain and best player , Rick Nash , requested to be traded , though he would remain with the team for the entire season . \n",
      "\n",
      "tensor([[50257,   383,  4518,  ...,   416,   597,  1074],\n",
      "        [  383,  4518, 41324,  ...,   597,  1074,   287],\n",
      "        [ 4518, 41324,  2540,  ...,  1074,   287,   281],\n",
      "        ...,\n",
      "        [ 8759, 13950,   837,  ...,  2104,  1622,   764],\n",
      "        [13950,   837,  9167,  ...,  1622,   764,   220],\n",
      "        [  837,  9167,   284,  ...,   764,   220,   198]])\n",
      "tensor([[  383,  4518, 41324,  ...,   597,  1074,   287],\n",
      "        [ 4518, 41324,  2540,  ...,  1074,   287,   281],\n",
      "        [41324,  2540,   262,  ...,   287,   281,  9481],\n",
      "        ...,\n",
      "        [13950,   837,  9167,  ...,  1622,   764,   220],\n",
      "        [  837,  9167,   284,  ...,   764,   220,   198],\n",
      "        [ 9167,   284,   307,  ...,   220,   198, 50258]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13402/1265923466.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  batched = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "example_sentence = train_dataset[100]\n",
    "print(example_sentence)\n",
    "\n",
    "token_full = np.concatenate(\n",
    "    ([SOS_TOKEN_ID], tokenizer(example_sentence).input_ids, [EOS_TOKEN_ID]), axis=-1,\n",
    ")\n",
    "token_full = np.expand_dims(token_full, axis=0)\n",
    "\n",
    "max_length = 20\n",
    "batched = torch.tensor(\n",
    "    [token_full[0][i:i+max_length] for i in range(0, len(token_full[0])-max_length)]\n",
    ")\n",
    "answer = torch.tensor(\n",
    "    [token_full[0][i:i+max_length] for i in range(1, len(token_full[0])-max_length+1)]\n",
    ")\n",
    "print(batched)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.013436317443848"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "model.train(example_sentence, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:23,  6.09s/it]wandb: ERROR Error while calling W&B API: run seung7361/GPT2TextGeneration/k5vvp2pv was previously created and deleted; try a new run name (<Response [409]>)\n",
      "4it [00:26,  6.61s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.15 GiB (GPU 3; 15.74 GiB total capacity; 13.72 GiB already allocated; 846.69 MiB free; 13.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m started.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m step, sentence \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_dataset)):\n\u001b[0;32m---> 10\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(sentence, loss_fn, optimizer)\n\u001b[1;32m     12\u001b[0m     wandb\u001b[39m.\u001b[39mlog({ \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: loss })\n",
      "Cell \u001b[0;32mIn[10], line 83\u001b[0m, in \u001b[0;36mGPTModel.train\u001b[0;34m(self, full_sentence, loss_fn, optimizer, max_length)\u001b[0m\n\u001b[1;32m     80\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(batched)\n\u001b[1;32m     81\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), answer\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     84\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     86\u001b[0m ret \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.15 GiB (GPU 3; 15.74 GiB total capacity; 13.72 GiB already allocated; 846.69 MiB free; 13.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: run seung7361/GPT2TextGeneration/k5vvp2pv was previously created and deleted; try a new run name (<Response [409]>)\n",
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/apis/normalize.py\", line 41, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\", line 1690, in upsert_run\n",
      "    response = self.gql(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\", line 257, in gql\n",
      "    ret = self._retry_gql(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/retry.py\", line 131, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py\", line 285, in execute\n",
      "    return self.client.execute(*args, **kwargs)  # type: ignore\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/gql_request.py\", line 56, in execute\n",
      "    request.raise_for_status()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://api.wandb.ai/graphql\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n",
      "    self._run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 93, in _run\n",
      "    self._debounce()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 334, in _debounce\n",
      "    self._sm.debounce()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 551, in debounce\n",
      "    self._maybe_update_config(always=final)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 528, in _maybe_update_config\n",
      "    self._debounce_config()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 557, in _debounce_config\n",
      "    self._api.upsert_run(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/wandb/apis/normalize.py\", line 51, in wrapper\n",
      "    raise CommError(message, error)\n",
      "wandb.errors.CommError: run seung7361/GPT2TextGeneration/k5vvp2pv was previously created and deleted; try a new run name (Error 409: Conflict)\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1} started.\")\n",
    "\n",
    "    for step, sentence in tqdm(enumerate(train_dataset)):\n",
    "        loss = model.train(sentence, loss_fn, optimizer)\n",
    "\n",
    "        wandb.log({ 'loss': loss })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
